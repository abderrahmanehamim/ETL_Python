{"cells":[{"source":"# **ETL IN PYTHON**","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"# **Downloading a ZIP file**\nYou now have a good understanding of the steps that make up our ETL pipeline. Now, let's get started implementing it.\n\nFirst thing first: you need to download the .zip file containing the new dataset that will be processed through the pipeline.\n\nThe path to the zipped file is saved in a variable called path: you can see it printed to the IPython shell.","metadata":{},"cell_type":"markdown","id":"5ecf253e-0efb-463a-88db-842398cee828"},{"source":"# Import the required library\nimport requests\n\n# Get the zip file\nresponse = requests.get(path)\n\n# Print the status code\nprint(response.status_code)\n\n# Save the file locally (more about open() in the next lesson)\nlocal_path = f\"tmp/data/source/downloaded_at=2021-02-01/PPR-ALL.zip\"\nwith open(local_path, \"wb\") as f:\n    f.write(response.content)","metadata":{},"id":"7f26dd6a-20aa-4d19-8d3f-b8f64fba1821","cell_type":"code","execution_count":1,"outputs":[]},{"source":"# **Exploring a ZIP file**\nYou just used requests to download a zipped file, which is now stored on your system. But what we actually need is the file it contains: a CSV storing data about real estate transactions and their characteristics.\n\nThe ZIP file path on your system is saved in a variable called path, that you can once again see printed to the IPython shell.","metadata":{},"cell_type":"markdown","id":"d410515f-b1e5-4931-b993-41c219291497"},{"source":"# Import the required method\nfrom zipfile import ZipFile\n\nwith ZipFile(path, mode=\"r\") as f:\n  \t# Get the list of files and print it\n    file_names = f.namelist()\n    print(file_names)","metadata":{},"cell_type":"code","id":"3cf55d47-f8bb-4824-8b13-bf6105cb3190","execution_count":null,"outputs":[]},{"source":"# Import the required method\nfrom zipfile import ZipFile\n\nwith ZipFile(path, \"r\") as f:\n    # Get the list of files\n    file_names = f.namelist()\n    print(file_names)\n    # Extract the CSV file\n    csv_file_path = f.extract(file_names[0])\n    print(csv_file_path)","metadata":{},"cell_type":"code","id":"e81a5aa4-7c43-4685-a7b6-489c61620976","execution_count":null,"outputs":[]},{"source":"# **Reading from a CSV file**\nYou're now going to explore a CSV file, PPR-2021-Dublin.csv, containing a subset of the data to be processed in the ETL pipeline.\n\nThe full path to the file is saved in a variable called path which is printed in the IPython shell on the bottom right.\n\nYou're going to open it in read mode and get familiar with its header and rows. You'll discover that each row is a dict object.\n\nYou may have noticed the from pprint import pprint statement at the top of your script. pprint() is a built-in Python function that basically prints a dictionary with each key-value pair on its own line, rather than all key-value pairs on one line. It simply makes the output more humanly readable.","metadata":{},"cell_type":"markdown","id":"31ead670-0033-49af-88e6-af0acbcc8c7b"},{"source":"import csv\nfrom pprint import pprint\n\n# Open the csv file in read mode\nwith open(path, mode=\"r\", encoding=\"windows-1252\") as csv_file:\n    # Open csv_file so that each row is a dictionary\n    reader = csv.DictReader(csv_file)\n    \n    # Print the first row\n    row = next(reader)\n    print(type(row))\n    pprint(row)","metadata":{},"cell_type":"code","id":"60d5b2ca-892e-449f-bd72-cbf26c5c63f5","execution_count":null,"outputs":[]},{"source":"# **Writing to CSV**\nYou just read a CSV file and printed the first row. You're now ready to edit the file header and save the rows in a new .csv file. Changing the file header for shorter column names without spaces will make managing columns more manageable in the long run.\n\nThen, you will write the header and the first row into a new CSV file named PPR-2021-Dublin-new-headers.csv.","metadata":{},"cell_type":"markdown","id":"3c3b49e8-b889-4190-b1e2-140fe5c6608d"},{"source":"import csv\n\nwith open(path, mode=\"r\", encoding=\"windows-1252\") as reader_csv_file:\n    reader = csv.DictReader(reader_csv_file)\n    # The new file is called \"PPR-2021-Dublin-new-headers.csv\"\n    # and will be saved inside the \"tmp\" folder    \n    with open(\"/tmp/PPR-2021-Dublin-new-headers.csv\",\n                    mode=\"w\",\n                    encoding=\"windows-1252\",\n                ) as writer_csv_file:\n        writer = csv.DictWriter(writer_csv_file, fieldnames=new_column_names)\n        # Write header as first line\n        writer.writerow(new_column_names)\n        for row in reader:\n\t        # Write all rows in file\n\t        writer.writerow(row)","metadata":{},"cell_type":"code","id":"ba37926d-1c2c-47d1-9fcd-47bb27711935","execution_count":null,"outputs":[]},{"source":"# **Downloading the new dataset file from web**\nSo far, you've downloaded a file, unzipped it, and read and wrote to a CSV. You've done all of this in independent, self-contained scripts. Now, you're going wrap it all into functions to design the Extract step of your ETL pipeline.\n\nYou now have access to an IDE (an Integrated Development Environment). Nothing to fear: instead of having just one script, you now have a whole directory around which you can navigate to manage your scripts and data. This directory is going to grow throughout the course as you write more scripts for the ETL pipeline.\n\nBut let's start with the beginning. You're going to define a first function, create_directory_if_not_exists(), to create the local directory where the data should be saved.\n\nYou will then define a second function, download_snapshot(), to download the zipped file containing the house transaction information. You will get the file from an external URL saved in a variable called source_url. You will save the zipped file locally at the location specified by the variable source_path, leveraging create_directory_if_not_exists().\n\nYou can see the source_url and source_path variables definitions in the script. You're also provided a base_path variable, which refers to the current working directory (/home/repl/workspace).","metadata":{},"cell_type":"markdown","id":"b5c8e169-49e9-4edf-a431-cd70d0e2ada7"},{"source":"import os\nimport requests\n\n# Paths\nbase_path = \"/home/repl/workspace\"\nsource_url = \"https://assets.datacamp.com/production/repositories/5899/datasets/66691278303f789ca4acd3c6406baa5fc6adaf28/PPR-ALL.zip\"\nsource_path =  f\"{base_path}/data/source/downloaded_at=2021-01-01/ppr-all.zip\"\n\n# Create a directory at the `path` passed as an argument\ndef create_directory_if_not_exists(path):\n    \"\"\"\n    Create a new directory if it doesn't exists\n    \"\"\"\n    # os.path.dirname() returns up to the directory path.\n    # In this case it is: f\"{base_path}/downloaded_at=2021-01-01\"\n    # \"ppr-all.zip\" is excluded\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n# Write the file obtained to the specified directory\ndef download_snapshot():\n    \"\"\"\n    Download the new dataset from the source\n    \"\"\"\n    create_directory_if_not_exists(source_path)\n    # Open the .zip file in binary mode\n    with open(source_path, mode=\"wb\") as source_ppr:\n        # 'verify=False' skips the verification the SSL certificate\n        response = requests.get(source_url, verify=False)\n        source_ppr.write(response.content)\n\n# Download the new dataset\ndownload_snapshot()","metadata":{},"cell_type":"code","id":"c7f76181-6c7e-496c-a8b3-92c9aea4fd16","execution_count":null,"outputs":[]},{"source":"# **Extract 'em all!**\nYou're now able to create a new directory to save the updated zipped file from an external source and save it locally inside the source directory. Great job!\n\nYou're close to completing the extract process. All that's left is getting the CSV file.\n\nThe January 2021 file has already been downloaded (see source/downloaded_at=2021-01-01/PPR-RAW.zip and raw/downloaded_at=2021-01-01/ppr-raw.csv). Now a new dataset (which contains new data) for February 2021 is available: you need to download it.\n\nNote that, in extract.py, you can see already functions responsible for:\n\n- downloading the ZIP file, and saving it in the source folder (download_snapshot())\n- extracting the CSV file and saving it in the raw folder (save_new_raw_data())\nIn the extract.py, you need to call the above snippets of code in the main() function. Then, you will start automating the pipeline by editing execute.py.\n\nNotice how the file structure is consistent in every step of the process. We move the file:\n\n- from source/downloaded_at=<YYYY-MM-DD>/PPR-all.zip\n- to raw/downloaded_at=<YYYY-MM-DD>/PPR-all.csv","metadata":{},"cell_type":"markdown","id":"5f7ba415-8d3f-4016-915c-b6ce1abcc443"},{"source":"# **Instructions**\n- Open extract.py and complete the main() function responsible to download both the ZIP and the extracted CSV files.\n- Open execute.py. Import the extract script (in Python, scripts are imported without using the .py extension in the import     \tstatement). Then, call its main() function.","metadata":{},"cell_type":"markdown","id":"427a2f87-4193-4bbe-af7b-37e9bbe32f67"},{"source":"import os\nimport csv\nimport tempfile\nfrom zipfile import ZipFile\n\nimport requests\n\n# Settings\nbase_path = os.path.abspath(__file__ + \"/../../\")\n\n# START - Paths for new February 2021 data available\n\n# External website file url\nsource_url = \"https://assets.datacamp.com/production/repositories/5899/datasets/66691278303f789ca4acd3c6406baa5fc6adaf28/PPR-ALL.zip\"\n\n# Source path where we want to save the .zip file downloaded from the website\nsource_path = f\"{base_path}/data/source/downloaded_at=2021-02-01/PPR-ALL.zip\"\n\n# Raw path where we want to extract the new .csv data\nraw_path = f\"{base_path}/data/raw/downloaded_at=2021-02-01/ppr-all.csv\"\n\n# END - Paths for new February 2021 data available\n\n\ndef create_folder_if_not_exists(path):\n    \"\"\"\n    Create a new folder if it doesn't exists\n    \"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n\ndef download_snapshot():\n    \"\"\"\n    Download the new dataset from the source\n    \"\"\"\n    create_folder_if_not_exists(source_path)\n    with open(source_path, \"wb\") as source_ppr:\n        response = requests.get(source_url, verify=False)\n        source_ppr.write(response.content)\n\n\ndef save_new_raw_data():\n    \"\"\"\n    Save new raw data from the source\n    \"\"\"\n\n    create_folder_if_not_exists(raw_path)\n    with tempfile.TemporaryDirectory() as dirpath:\n        with ZipFile(\n            source_path,\n            \"r\",\n        ) as zipfile:\n            names_list = zipfile.namelist()\n            csv_file_path = zipfile.extract(names_list[0], path=dirpath)\n            # Open the CSV file in read mode\n            with open(csv_file_path, mode=\"r\", encoding=\"windows-1252\") as csv_file:\n                reader = csv.DictReader(csv_file)\n\n                row = next(reader)  # Get first row from reader\n                print(\"[Extract] First row example:\", row)\n\n                # Open the CSV file in write mode\n                with open(\n                    raw_path,\n                    mode=\"w\",\n                    encoding=\"windows-1252\"\n                ) as csv_file:\n                    # Rename field names so they're ready for the next step\n                    fieldnames = {\n                        \"Date of Sale (dd/mm/yyyy)\": \"date_of_sale\",\n                        \"Address\": \"address\",\n                        \"Postal Code\": \"postal_code\",\n                        \"County\": \"county\",\n                        \"Price (€)\": \"price\",\n                        \"Description of Property\": \"description\",\n                    }\n                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n                    # Write headers as first line\n                    writer.writerow(fieldnames)\n                    for row in reader:\n                        # Write all rows in file\n                        writer.writerow(row)\n\n# Main function called inside the execute.py script\ndef main():\n    print(\"[Extract] Start\")\n    print(\"[Extract] Downloading snapshot\")\n    download_snapshot()\n    print(f\"[Extract] Saving data from '{source_path}' to '{raw_path}'\")\n    save_new_raw_data()\n    print(f\"[Extract] End\")","metadata":{},"cell_type":"code","id":"e9201258-4307-4cf5-8c82-8f5c5f01be65","execution_count":null,"outputs":[]},{"source":"import extract\n\nif __name__ == \"__main__\":\n    extract.main()","metadata":{},"cell_type":"code","id":"5720020f-ab64-4757-8fe1-d7e0f07f7358","execution_count":null,"outputs":[]},{"source":"# **SQLAlchemy core components**\nYou now know about SQLAlchemy engines and sessions, the two core components needed to work and communicate with SQL databases in Python.\n\n### _**\n### An engine is used to manage SQL dialects and connectors. It lets you interact with a database while a session establishes all conversations with the database and represents a \"holding area\" before committing all changes to the database.**_","metadata":{},"cell_type":"markdown","id":"adda2c6c-eaf0-4042-9cb2-98a6bc3ffa75"},{"source":"# **Engines and sessions**\nAs you know by now, engines and sessions are key components enabling SQLAlchemy to interact with a database.\n\nSo let's create an engine and bind it to a session.\n\nRemember:\n\npostgresql is the dialect\npsycopg2 is the connector\nyou're working on a local server, localhost","metadata":{},"cell_type":"markdown","id":"5692d12e-9148-45b6-affa-0964445af357"},{"source":"# Import the function needed\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\n\n# Create the engine\nengine = create_engine(\"postgresql+psycopg2://dcstudent:S3cretPassw0rd@localhost:5432/campdata-prod\")\n\n# Create the session\nsession = Session(engine)","metadata":{},"cell_type":"code","id":"5541becc-93b7-43ae-8355-fcf0ee9d92fe","execution_count":null,"outputs":[]},{"source":"# **Table class definition**\nYou already know that you can use engines and sessions to connect and edit a database. Now, you're going to define a Python class to create a SQL table.\n\nBut first, we need to map the Python table class with the database table called ppr_raw_all.","metadata":{},"cell_type":"markdown","id":"59d82113-37d4-4ae8-8a65-9b0b1c3e83b3"},{"source":"# Import the objects needed\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer\n\n# Initialize the base and set inheritance\nBase = declarative_base()\n\nclass PprRawAll(Base):\n    # Set the table name\n    __tablename__ = \"ppr_raw_all\"\n    # Create a primary key integer column id\n    id = Column(Integer, primary_key=True)","metadata":{},"cell_type":"code","id":"ef05d9b7-e869-4bda-ae8d-ddaca28bbc43","execution_count":null,"outputs":[]},{"source":"# **Columns definition**\nYou just mapped a Python class to a PostgreSQL table, declaring a table name and a primary key.\n\nYou can now declare the rest of the columns, according to this table definition: Table name: ppr_raw_all\n\n- Column name\ttype\n- id\tinteger\n- date_of_sale\tvarchar(55)\n- address\tvarchar(255)\n- postal_code\tvarchar(55)\n- county\tvarchar(55)\n- price\tvarchar(55)\n- description\tvarchar(55)","metadata":{},"cell_type":"markdown","id":"f40cedd7-a6af-4c95-b11a-4481c1c93694"},{"source":"from sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer, String\n\nBase = declarative_base()\n\nclass PprRawAll(Base):\n    __tablename__ = \"ppr_raw_all\"\n    \n    id = Column(Integer, primary_key=True)\n    date_of_sale = Column(String(55))\n    address = Column(String(255))\n    postal_code = Column(String(55))\n    county = Column(String(55))\n    price = Column(String(55))\n    description = Column(String(55))","metadata":{},"cell_type":"code","id":"edb903c9-74fb-4310-903a-b25152d9f95a","execution_count":null,"outputs":[]},{"source":"# **Lower string and date format**\nYou now need to prepare and transform the data.\n\nIn particular, you're going to start by defining two functions:\n\ntransform_case() to lowercase strings, taking a generic string as an argument\nupdate_date_of_sale(), to update the date format, taking a date format string (e.g. 12/02/2021) as an argument","metadata":{},"cell_type":"markdown","id":"20071483-8feb-4756-a19f-6a55a272d848"},{"source":"# Import the submodule required\nfrom datetime import datetime\n\ndef transform_case(input_string):\n    \"\"\"\n    Lowercase string fields\n    \"\"\"\n    # Return the string lowercase\n    return input_string.lower()\n  \ndef update_date_of_sale(date_input):\n    \"\"\"\n    Update date format from DD/MM/YYYY to YYYY-MM-DD\n    \"\"\"\n    # Create a datetime object\n    current_format = datetime.strptime(date_input, \"%d/%m/%Y\")\n    # Convert to the expected date format\n    new_format = current_format.strftime(\"%Y-%m-%d\")\n    return new_format","metadata":{},"cell_type":"code","id":"e567791e-88d7-4d43-956a-ee33d94452df","execution_count":null,"outputs":[]},{"source":"# **Price and description**\nStrings can now be lowered and dates can be converted to the expected format. Let's now clean the price and description columns.\n\nYou're going to develop two functions:\n\nupdate_price(), which takes a string like €200,000.00 as input and returns an integer\nupdate_description(), which gets a description (\"second-hand dwelling house /apartment\" or \"new dwelling house /apartment\") as input and returns a new or second-hand.","metadata":{},"cell_type":"markdown","id":"fac58550-d8b8-4c39-90e2-318782306eb0"},{"source":"from datetime import datetime\n\ndef transform_case(input_string):\n    \"\"\"\n    Lowercase string fields\n    \"\"\"\n    return input_string.lower()\n  \ndef update_date_of_sale(date_input):\n    \"\"\"\n    Updates date format from DD/MM/YYYY to YYYY-MM-DD\n    \"\"\"\n    current_format = datetime.strptime(date_input, \"%d/%m/%Y\")\n    new_format = current_format.strftime(\"%Y-%m-%d\")\n    return new_format\n\ndef update_price(price_input):\n    \"\"\"\n    Returns price as an integer by removing:\n    - \"€\" and \",\" symbol\n    - Converting to float first then to integer\n    \"\"\"\n    # Replace € with an empty string\n    price_input = price_input.replace(\"€\", \"\")\n    # Replace comma with an empty string\n    price_input = price_input.replace(\",\", \"\")\n    # Convert to float\n    price_input = float(price_input)\n    # Return price_input as integer\n    return int(price_input)\n  \ndef update_description(description_input):\n    \"\"\"\n    Simplifies the description field for future analysis. Returns:\n    - \"new\" if string contains \"new\" substring\n    - \"second-hand\" if string contains \"second-hand\" substring\n    \"\"\"\n    description_input = transform_case(description_input)\n    # Check description and return \"new\" or \"second-hand\"\n    if \"new\" in description_input:\n        return \"new\"\n    elif \"second-hand\" in description_input:\n        return \"second-hand\"\n    return description_input","metadata":{},"cell_type":"code","id":"fcb5c606-60f0-4a28-8b87-b9805d63fea0","execution_count":null,"outputs":[]},{"source":"# **Setup base script**\nYou know how to connect to a database, apply changes to a CSV, and create tables. Now you're going to bring all of this together.\n\nYour first action item is to create a base.py file. it will contain the engine and base that other scripts will rely upon.","metadata":{},"cell_type":"markdown","id":"0e9d2822-4498-42fc-ad17-ac481cf98773"},{"source":"# **Instruction**\n- Import the modules you need to create engines, set up sessions, and declare bases.\n- Create the engine, using the following URI: postgresql+psycopg2://dcstudent:S3cretPassw0rd@localhost:5432/campdata-prod.\n- Initialize the session object.\n- Initialize the declarative base.","metadata":{},"cell_type":"markdown","id":"726721fd-228f-4b6d-887c-3d96cd865852"},{"source":"# Import the modules required\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import declarative_base, Session\n\n# Create the engine\nengine = create_engine(\"postgresql+psycopg2://dcstudent:S3cretPassw0rd@localhost:5432/campdata-prod\")\n\n# Initialize the session\nsession = Session(engine)\n\n# Initialize the declarative base\nBase = declarative_base()","metadata":{},"cell_type":"code","id":"dd2d0b7c-6143-4964-b709-c11821b4492a","execution_count":null,"outputs":[]},{"source":"# **Create tables**\nHaving set up your engine, session and base in base.py, you're ready to create your first table and commit changes directly to the PostgreSQL database. You will do so in create_tables.py.\n\nRemember that the base object has metadata about the schema and tables. It can also call methods to create missing tables.","metadata":{},"cell_type":"markdown","id":"f1d99234-c540-42f3-a462-4a676c594f32"},{"source":"# **Instruction**\n- Import the PprRawAll class corresponding to the table you want to create in the SQL database. The table class is defined in common/tables.py.\n- Use the metadata object from Base to create the table. Remember that you need to establish a connection to the database in order to push changes.\n- \n","metadata":{},"cell_type":"markdown","id":"4a05383c-566b-4865-982a-7e348758744d"},{"source":"from base import Base, engine\n# Import the PprRawAll table\nfrom tables import PprRawAll\n\n# Create the table in the database\nif __name__ == \"__main__\":\n    Base.metadata.create_all(engine)","metadata":{},"cell_type":"code","id":"e13111ab-2088-41cf-8125-1a0aebc715a6","execution_count":null,"outputs":[]},{"source":"# **Transform 'em all!**\nAt this point, you have set up utilities to:\n\nchange the date format, manipulate strings, update values, and more\nconnect to a PostgreSQL database using Python\nmap Python classes to PostgreSQL tables\nYou have an existing empty table: now it's time to wrap it all up and save the transformed data in the previously created PprRawAll table.\n\nYou have to work on the transform.py script and, in particular, the main() function.\n\nSome information to keep in mind:\n\nWe're moving the data from a CSV file to a table called PprRawAll.\nThis will make it easy to make transformation on the data by using python and SQL.\nThe transform_new_data() function is responsible to apply all transformations and save the newly updated rows into the database table PprRawAll.\nThe end result for ppr_raw_all table in our database will be something like the following:\n\nPprRawAll table content\n\nYou might also notice the function truncate_table(), which you're seeing for the first time. It ensures that the ppr_raw_all table is empty every time the script is called (for example to avoid any inconsistent state if a job fails).","metadata":{},"cell_type":"markdown","id":"60533deb-06b0-4409-a05d-c55db0976f06"},{"source":"# Instruction\n- Just as you did at the end of Chapter 1, complete the main() function. First, you need to truncate the table and then run all the transformations.\n- Edit execute.py to import the transform script and call its main function, enabling you to automate the process.","metadata":{},"cell_type":"markdown","id":"05939856-cc56-44c2-aa67-e9f5d7c7934d"},{"source":"# transform.py","metadata":{},"cell_type":"markdown","id":"09089cb3-5c3d-4459-9f5d-239e342370d3"},{"source":"import os\nimport csv\nfrom datetime import datetime\n\nfrom common.tables import PprRawAll\nfrom common.base import session\nfrom sqlalchemy import text\n\n# Settings\nbase_path = os.path.abspath(__file__ + \"/../../\")\n\n# START - Paths for new February 2021 data available\n\n# Raw path where we want to extract the new CSV data\nraw_path = f\"{base_path}/data/raw/downloaded_at=2021-02-01/ppr-all.csv\"\n\n# END - Paths for new February 2021 data available\n\n\ndef transform_case(input_string):\n    \"\"\"\n    Lowercase string fields\n    \"\"\"\n    return input_string.lower()\n\n\ndef update_date_of_sale(date_input):\n    \"\"\"\n    Update date format from DD/MM/YYYY to YYYY-MM-DD\n    \"\"\"\n    current_format = datetime.strptime(date_input, \"%d/%m/%Y\")\n    new_format = current_format.strftime(\"%Y-%m-%d\")\n    return new_format\n\n\ndef update_description(description_input):\n    \"\"\"\n    Simplify the description field for potentialy future analysis, just return:\n    - \"new\" if string contains \"new\" substring\n    - \"second-hand\" if string contains \"second-hand\" substring\n    \"\"\"\n    description_input = transform_case(description_input)\n    if \"new\" in description_input:\n        return \"new\"\n    elif \"second-hand\" in description_input:\n        return \"second-hand\"\n    return description_input\n\n\ndef update_price(price_input):\n    \"\"\"\n    Return price as integer by removing:\n    - \"€\" symbol\n    - \",\" to convert the number into float first (e.g. from \"€100,000.00\" to \"100000.00\")\n    \"\"\"\n    price_input = price_input.replace(\"€\", \"\")\n    price_input = float(price_input.replace(\",\", \"\"))\n    return int(price_input)\n\n\ndef truncate_table():\n    \"\"\"\n    Ensure that \"ppr_raw_all\" table is always in empty state before running any transformations.\n    And primary key (id) restarts from 1.\n    \"\"\"\n    session.execute(\n        text(\"TRUNCATE TABLE ppr_raw_all;ALTER SEQUENCE ppr_raw_all_id_seq RESTART;\")\n    )\n    session.commit()\n\n\ndef transform_new_data():\n    \"\"\"\n    Apply all transformations for each row in the .csv file before saving it into database\n    \"\"\"\n    with open(raw_path, mode=\"r\", encoding=\"windows-1252\") as csv_file:\n        # Read the new CSV snapshot ready to be processed\n        reader = csv.DictReader(csv_file)\n        # Initialize an empty list for our PprRawAll objects\n        ppr_raw_objects = []\n        for row in reader:\n            # Apply transformations and save as PprRawAll object\n            ppr_raw_objects.append(\n                PprRawAll(\n                    date_of_sale=update_date_of_sale(row[\"date_of_sale\"]),\n                    address=transform_case(row[\"address\"]),\n                    postal_code=transform_case(row[\"postal_code\"]),\n                    county=transform_case(row[\"county\"]),\n                    price=update_price(row[\"price\"]),\n                    description=update_description(row[\"description\"]),\n                )\n            )\n        # Save all new processed objects and commit\n        session.bulk_save_objects(ppr_raw_objects)\n        session.commit()\n\n\ndef main():\n    print(\"[Transform] Start\")\n    print(\"[Transform] Remove any old data from ppr_raw_all table\")\n    truncate_table()\n    print(\"[Transform] Transform new data available in ppr_raw_all table\")\n    transform_new_data()\n    print(\"[Transform] End\")","metadata":{},"cell_type":"code","id":"f78f2cee-4cd3-44ac-92b6-926f58c697e9","execution_count":null,"outputs":[]},{"source":"# execute.py","metadata":{},"cell_type":"markdown","id":"391a3b9f-e412-442e-8e25-56cfb1b7d8f4"},{"source":"# Import the transform script\nimport extract\nimport transform\n\n# Call its main function\nif __name__ == \"__main__\":\n    extract.main()\n    transform.main()  ","metadata":{},"cell_type":"code","id":"dd1fd095-5973-4904-b98a-b41cfb337ffb","execution_count":null,"outputs":[]},{"source":"# **Date data type definition**\nYou've been introduced to two new concepts: the date data type, and the column uniqueness constraint. You've also learned about the existence of another table, the clean table.\n\nJust as the raw table refers to ppr_raw_all, the clean table refers ppr_clean_all, which stores the complete and updated dataset used by analysts.\n\nLet's start by defining a new column data type for the table ppr_clean_all.\n\nYou're going to work in the tables.py and create_tables.py files.","metadata":{},"cell_type":"markdown","id":"e892e7bf-fe85-4b84-8860-90f5114fe0b2"},{"source":"# Create_tables.py","metadata":{},"cell_type":"markdown","id":"9a00c21d-dc8b-4951-8e8d-bd69a6a17891"},{"source":"from base import Base, engine\n# Import the class corresponding to the clean table\nfrom tables import PprRawAll, PprCleanALL\n\nif __name__ == \"__main__\":\n    Base.metadata.create_all(engine)","metadata":{},"cell_type":"code","id":"619a4fad-7c26-4279-8db1-2cb8b3d0baa2","execution_count":null,"outputs":[]},{"source":"# tables.py","metadata":{},"cell_type":"markdown","id":"24ec51f5-dfe8-4679-855c-c9c12746829a"},{"source":"# Import the class you need\nfrom sqlalchemy import Column, Integer, String, Date\n\nfrom base import Base\n\n\nclass PprRawAll(Base):\n    __tablename__ = \"ppr_raw_all\"\n\n    id = Column(Integer, primary_key=True)\n    date_of_sale = Column(String(55))\n    address = Column(String(255))\n    postal_code = Column(String(55))\n    county = Column(String(55))\n    price = Column(String(55))\n    description = Column(String(255))\n\nclass PprCleanAll(Base):\n    __tablename__ = \"ppr_clean_all\"\n\n    id = Column(Integer, primary_key=True)\n    # Create a new column of type Date\n    date_of_sale = Column(Date)\n    address = Column(String(255))\n    postal_code = Column(String(55))\n    county = Column(String(55))\n    price = Column(Integer)\n    description = Column(String(255))","metadata":{},"cell_type":"code","id":"00c99830-c712-4d70-a773-44d370ce0a06","execution_count":null,"outputs":[]},{"source":"# Unique key definition\nYou just added a Date column. Now it's time to create the unique column needed for insert and delete operations between the raw table and the clean one.\n\nThe unique key should be a string called transaction_id. It will be a concatenation of four columns (date_of_sale, address, county and price) separated by a dash (_).\n\nYou're going to define transaction_id as a column_property for both PprRawAll and PprCleanAll.\n\nLet's take this row as an example:\n\n- date_of_sale\taddress\tpostal_code\tcounty\tprice\tdescription\n- 2021-02-12\t123 walkinstown park, walkinstown, dublin 12\tdublin 12\tdublin\t297000\tsecond-hand\n- Its transaction_id value will be \"2021-02-12_123 walkinstown park, walkinstown, dublin 12_dublin_297000\".\n\nYou're going to work in tables.py.","metadata":{},"cell_type":"markdown","id":"52b68933-5f72-4743-90cf-3bb06f650cb2"},{"source":"# Instruction\n- Import the function you need to create a unique key.\n- Define the transaction_id on PprRawAll as a column property with the needed columns.\n- Define the transaction_id on PprCleanAll as a column property with the needed columns. Some of the columns will need to be converted to string format.","metadata":{},"cell_type":"markdown","id":"7b762449-d740-4ca6-af47-d4f21d5eb9ba"},{"source":"from sqlalchemy import cast, Column, Integer, String, Date\n# Import the function required\nfrom sqlalchemy.orm import column_property\n\nfrom base import Base\n\nclass PprRawAll(Base):\n    __tablename__ = \"ppr_raw_all\"\n\n    id = Column(Integer, primary_key=True)\n    date_of_sale = Column(String(55))\n    address = Column(String(255))\n    postal_code = Column(String(55))\n    county = Column(String(55))\n    price = Column(String(55))\n    description = Column(String(255))\n    # Create a unique transaction id\n    transaction_id = column_property(\n        date_of_sale + \"_\" + address + \"_\" + county + \"_\" + price\n    )\n\nclass PprCleanAll(Base):\n    __tablename__ = \"ppr_clean_all\"\n\n    id = Column(Integer, primary_key=True)\n    date_of_sale = Column(Date)\n    address = Column(String(255))\n    postal_code = Column(String(55))\n    county = Column(String(55))\n    price = Column(Integer)\n    description = Column(String(255))\n    # Create a unique transaction id\n    # all non-string columns are casted as string\n    transaction_id = column_property(\n        cast(date_of_sale, String) + \"_\" + address + \"_\" + county + \"_\" + cast(price, String)\n    )","metadata":{},"cell_type":"code","id":"fc0db540-2bd7-48fd-b669-6cc38ccb207f","execution_count":null,"outputs":[]},{"source":"# Querying\nYou've previously defined a PprRawAll class corresponding to the raw table. Now you're going to query it.\n\nYou need to retrieve the row with id equal to 2 and print the corresponding address value.\n\nsession and PprRawAll classes are already initialized and ready to use.","metadata":{},"cell_type":"markdown","id":"998c4a59-a499-4093-9a7e-e91f247b3d6d"},{"source":"# Instruction\n- Query the PprRawAll table, and filter for the row whose id column value is equal to 2.\n- Print the address from the first row.","metadata":{},"cell_type":"markdown","id":"660a9e9a-2a76-4b8d-be42-f65234ec62a3"},{"source":"# Query the session to get row with id equal to 2\nresults = session.query(PprRawAll).filter(PprRawAll.id == 2)\n\n# Get the corresponding address\nprint(\"Address:\", results[0].address)","metadata":{},"cell_type":"code","id":"0ca3b508-77f0-41a1-a308-344d7c8e95a3","execution_count":null,"outputs":[]},{"source":"# **Insert and delete**\nLet's now focus on our load phase. It relies heavily on two fundamental operations:\n\ninserting rows in a table\ndeleting rows from a table\nsession and PprCleanAll classes are already initialized and re","metadata":{},"cell_type":"markdown","id":"cbb23d0b-b7f3-4e86-828a-bcef80d986dc"},{"source":"# Instruction\nImport the function you need to insert rows.\nInsert the predefined values in the PprCleanAll table.\nExecute the statement and [commit.]()","metadata":{},"cell_type":"markdown","id":"a5caa8de-8aee-4f56-9d96-835c205d38d0"},{"source":"# Import the function required\nfrom sqlalchemy.dialects.postgresql import insert\n  \nvalues = [{\"date_of_sale\": \"2021-01-01\",\n           \"address\": \"14 bow street\",\n           \"postal_code\": \"dublin 7\",\n           \"county\": \"dublin\",\n           \"price\": 350000,\n           \"description\":\"second-hand\"}]\n\n# Insert values in PprCleanAll\nstmt = insert(PprCleanAll).values(values)\n\n# Execute and commit\nsession.execute(stmt)\nsession.commit()","metadata":{},"cell_type":"code","id":"1ac5a574-bd7c-4f1e-925d-a49e9e8e012f","execution_count":null,"outputs":[]},{"source":"# Instruction\nImport the function you need to delete rows.\nDelete all the rows that don't have a description.\nExecute the statement and commit.","metadata":{},"cell_type":"markdown","id":"7b3da93e-0512-4e81-950c-e91600d7197a"},{"source":"# Import the function required\nfrom sqlalchemy import delete\n\n# Delete rows lacking a description value\nstmt = delete(PprCleanAll).filter(PprCleanAll.description==\"\")\n\n# Execute and commit\nsession.execute(stmt)\nsession.commit()","metadata":{},"cell_type":"code","id":"5d7775fc-a493-467d-b5f8-0e2d9d17a512","execution_count":null,"outputs":[]},{"source":"# Insert operation\nTime to insert some data in the clean table! You're going to build upon the previous exercises and add all the new rows that are in the raw table, but not yet in the clean one.\n\nThe session, PprRawAll and PprCleanAll classes are already initialized and ready to use.","metadata":{},"cell_type":"markdown","id":"6fc8494f-5c31-4631-bf97-65783491a5c7"},{"source":"# Instructions\n- Select the transaction ids from the clean table.\n- Select the columns that need to be inserted: date of sale, address, postal code, county, price and description. The date of sale should be a date and the price an integer.\n- Filter for the new rows (rows that are in the raw table, but not in the clean one).\n- Use the list of new rows to complete the insert statement.","metadata":{},"cell_type":"markdown","id":"527ad317-edfe-4d12-a6b6-e8c86c5f5110"},{"source":"from sqlalchemy import cast\nfrom sqlalchemy.dialects.postgresql import insert\n\n# Select the transaction ids\nclean_transaction_ids = session.query(PprCleanAll.transaction_id)\n\n# Select the columns and cast the appropriate types if needed\ntransactions_to_insert = session.query(\n    cast(PprRawAll.date_of_sale, Date),\n    PprRawAll.address,\n    PprRawAll.postal_code,\n    PprRawAll.county,\n    cast(PprRawAll.price, Integer),\n    PprRawAll.description,\n  # Filter for the new rows\n).filter(~PprRawAll.transaction_id.in_(clean_transaction_ids))\n\n# Print total number of transactions to insert\n# it should be 3154 if the transactions need to be inserted\n# 0, if all transactions have been inserted\nprint(\"Transactions to insert:\", transactions_to_insert.count())\n\n# Insert the rows from the previously selected transactions\ncolumns = [\"date_of_sale\", \"address\", \"postal_code\",\n          \"county\", \"price\",\"description\"]\nstm = insert(PprCleanAll).from_select(columns, transactions_to_insert)\n\n# Execute and commit the statement to make changes in the database.\nsession.execute(stm)\nsession.commit()","metadata":{},"cell_type":"code","id":"bbbb21d8-9335-4ff0-a0b5-0f55cfbeb105","execution_count":null,"outputs":[]},{"source":"# Delete operation\nAfter inserting new rows from the raw table to the clean table, you need to remove rows that are in the clean table but no longer in the raw one.\n\nThe session, PprRawAll and PprCleanAll classes are already initialized and ready to use.","metadata":{},"cell_type":"markdown","id":"7f2fc8ec-bc3d-476c-ac94-bc8c0cbb8c87"},{"source":"# Instruction\n- Get the list of all transactions from the ppr_raw_all table.\n- Query the clean table, and filter out the rows that are in the clean table but not in the list of of raw transactions you just created.\n- Delete these rows.","metadata":{},"cell_type":"markdown","id":"53193fb9-d31f-4629-b6ca-44289dc9ceb0"},{"source":"# Import the delete module\nfrom sqlalchemy import delete\n\n# Get all the ppr_raw_all transaction ids\nraw_transaction_ids = session.query(PprRawAll.transaction_id)\n\n# Filter all the ppr_clean_all table transactions that are not present in the ppr_raw_all table\ntransactions_to_delete = session.query(PprCleanAll).filter(~PprCleanAll.transaction_id.in_(raw_transaction_ids))\n\n# Print transactions to delete\nprint(\"Transactions to delete:\", transactions_to_delete.count())\n\n# Delete the selected transactions\n# (Please note: the param \"synchronize_session=False\" has been inserted\n# to avoid inconsistent results if a session expires)\ntransactions_to_delete.delete(synchronize_session=False)\n\n# Commit the session to make the changes in the database\nsession.commit()","metadata":{},"cell_type":"code","id":"0112e075-fe7e-44f9-aaf0-7faf0c251093","execution_count":null,"outputs":[]},{"source":"# Load 'em all!\nYou're now ready to complete the last step of your ETL pipeline, and bring everything you learned in this chapter together.\n\nNotice that the insert and delete operations you just wrote have been wrapped into the corresponding functions insert_transactions() and delete_transactions().\n\nYou now need to make sure the load operation can be automated, like you did for the extract and transform operations at the end of Chapters 1 and 2.","metadata":{},"cell_type":"markdown","id":"29f0cfb9-9583-4533-96b8-a1197dff9992"},{"source":"# Instruction\n- Just as you did at the end of Chapter 1, complete the load.py main() function.\n- Then, edit execute.py to import the load script and call its main function, enabling you to automate the process.","metadata":{},"cell_type":"markdown","id":"33caacb6-7c74-43b7-a4e8-7bef1296d2ca"},{"source":"# Load.py","metadata":{},"cell_type":"markdown","id":"8c86035e-341e-4a66-b5c3-8b6728c00265"},{"source":"from common.base import session\nfrom common.tables import PprRawAll, PprCleanAll\n\nfrom sqlalchemy import cast, Integer, Date\nfrom sqlalchemy.dialects.postgresql import insert\n\n\ndef insert_transactions():\n    \"\"\"\n    Insert operation: add new data\n    \"\"\"\n    # Retrieve all the transaction ids from the clean table\n    clean_transaction_ids = session.query(PprCleanAll.transaction_id)\n\n    # date_of_sale and price needs to be casted as their\n    # datatype is not string but, respectively, Date and Integer\n    transactions_to_insert = session.query(\n        cast(PprRawAll.date_of_sale, Date),\n        PprRawAll.address,\n        PprRawAll.postal_code,\n        PprRawAll.county,\n        cast(PprRawAll.price, Integer),\n        PprRawAll.description,\n    ).filter(~PprRawAll.transaction_id.in_(clean_transaction_ids))\n\t\n    # Print total number of transactions to insert\n    print(\"Transactions to insert:\", transactions_to_insert.count())\n    \n    # Insert the rows from the previously selected transactions\n    stm = insert(PprCleanAll).from_select(\n        [\"date_of_sale\", \"address\", \"postal_code\", \"county\", \"price\", \"description\"],\n        transactions_to_insert,\n    )\n\n    # Execute and commit the statement to make changes in the database.\n    session.execute(stm)\n    session.commit()\n\n\ndef delete_transactions():\n    \"\"\"\n    Delete operation: delete any row not present in the last snapshot\n    \"\"\"\n    # Get all ppr_raw_all transaction ids\n    raw_transaction_ids = session.query(PprRawAll.transaction_id)\n\n    # Filter all the ppt_clean_all table transactions that are not present in the ppr_raw_all table\n    # and delete them.\n    # Passing synchronize_session as argument for the delete method.\n    transactions_to_delete = session.query(PprCleanAll).filter(\n        ~PprCleanAll.transaction_id.in_(raw_transaction_ids)\n    )\n    \n    # Print transactions to delete\n    print(\"Transactions to delete:\", transactions_to_delete.count())\n\n    # Delete transactions\n    transactions_to_delete.delete(synchronize_session=False)\n\n    # Commit the session to make the changes in the database\n    session.commit()\n\ndef main():\n    print(\"[Load] Start\")\n    print(\"[Load] Inserting new rows\")\n    insert_transactions()\n    print(\"[Load] Deleting rows not available in the new transformed data\")\n    delete_transactions\n    print(\"[Load] End\")","metadata":{},"cell_type":"code","id":"768864b8-5f8e-40f9-b772-3758a58b6a6f","execution_count":null,"outputs":[]},{"source":"# execute.py","metadata":{},"cell_type":"markdown","id":"e0b1a22e-f71a-4897-ba8b-bc2dbd99e349"},{"source":"import extract\nimport transform\nimport load\n\nif __name__ == \"__main__\":\n    extract.main()\n    transform.main()\n    load.main()","metadata":{},"cell_type":"code","id":"67490f18-df5f-455c-a0ac-4dab1079956b","execution_count":null,"outputs":[]},{"source":"# Sales for Dublin and Cork\nOne of the DCG Capital shareholders wants to get all the sales transactions for properties located in the Dublin or Cork counties.\n\nThe session and PprCleanAll class are already initialized and ready to use.\n\n**Instructions**\n\n- Import the operator you need to get sales transactions in the Dublin or Cork counties.\n- Query all the requested transactions.\n- Filter to get only Dublin and Cork.","metadata":{},"cell_type":"markdown","id":"ac5b9dec-b384-433c-bc19-c16647bade80"},{"source":"# Import the operator you need\nfrom sqlalchemy import or_\n\n# Query the clean table to retrieve the total number of\n# transactions for the Dublin or Cork counties\nresult = session.query(PprCleanAll) \\\n                .filter(or_(PprCleanAll.county == \"dublin\", PprCleanAll.county == \"cork\")) \\\n                .all()\n\nprint(\"First row address:\", result[0].address)","metadata":{},"cell_type":"code","id":"0340d468-71d6-4c04-8426-79b468325355","execution_count":null,"outputs":[]},{"source":"# First month 2021 sales\nPleased by your previous report, the same shareholder immediately asks you to list all the sales transactions that happened in January 2021.\n\nThe session and PprCleanAll class are already initialized and ready to use.\n\n**Instructions**\n\n- Import the operator needed.\n- Query all the requested transactions.\n- Filter for January sales only: from 2021-01-01 to 2021-01-31.\n\n","metadata":{},"cell_type":"markdown","id":"26f7270a-8683-4cb2-bcb2-2aa093f67a06"},{"source":"# Import the and function needed\nfrom sqlalchemy import and_\n\n# Retrieve all sales transactions for January 2021\nresult = session.query(PprCleanAll).filter(and_(PprCleanAll.date_of_sale >= \"2021-01-01\", PprCleanAll.date_of_sale <= \"2021-01-31\")).all()\n\nprint(\"First row address:\", result[0].address)","metadata":{},"cell_type":"code","id":"6030b81d-fab1-4621-bbbd-67569bc4d862","execution_count":null,"outputs":[]},{"source":"# Average, max and min functions\nLet's consider the following product table:\n\n- id\tcategory\tname\tprice\n- 1\tbooks\tSapiens\t12\n- 2\telectronics\tiPhone 12\t900\n- 3\tbooks\tMeasure What Matters\t10\n- 4\tbooks\tGreenlights\t14\n- 5\telectronics\tMacbook Pro 13\t1500\n- Your goal is to calculate the maximum, minimum and average values of these products' prices, for each category.\n\nThe session and Products classes are already defined.\n\n**Instructions**\n\n- Import the submodule you need to perform aggregate functions in SQLAlchemy.\n- Calculate the maximum, minimum and average values of the price column.\n- Make sure you get these aggregates for each category of product.","metadata":{},"cell_type":"markdown","id":"9d8b4777-b3fe-4504-b69c-896808b22dbf"},{"source":"# Import the submodule required\nfrom sqlalchemy import func\n\n# Get the maximum, minimum, and average values for each product category\nresult = session.query(Products.category,\n                       func.max(Products.price),\n                       func.min(Products.price),\n                       func.avg(Products.price)) \\\n                .group_by(Products.category).all()\n\nprint(\"Result:\", result)","metadata":{},"cell_type":"code","id":"b56b3d8e-0b4a-4e2e-9466-f9d5abcabf03","execution_count":null,"outputs":[]},{"source":"# Creating the insights view\nYou've been tasked with creating a view called insights that contains insights shareholders are regularly asking for.\n\n**Instructions**\n\n- Create the new view insights or replace it if it already exists.\n- The view needs to indicate the total number of sales, the sales total, the max sale, the min sale and the sales average.\n- Execute the query to create the view, then commit the changes to the database.","metadata":{},"cell_type":"markdown","id":"f93f6f63-1f31-4907-bd3e-c65eb9014eb9"},{"source":"from common.base import session\n\n# Create the view with the appropriate metrics\nquery = \"\"\"\nCREATE OR Replace view insights AS\nSELECT county,\n       Count(*) AS sales_count,\n       SUM(CAST(price AS int)) AS sales_total,\n       MAX(CAST(price AS int)) AS sales_max,\n       MIN(CAST(price AS int)) AS sales_min,\n       AVG(CAST(price AS int))::numeric(10,2) AS sales_avg\nFROM ppr_clean_all\nGROUP BY county\n\"\"\"\n\n# Execute and commit\nsession.execute(query)\nsession.commit()","metadata":{},"cell_type":"code","id":"0a08ed21-0c59-44d3-b8b8-f13f8b3f534a","execution_count":null,"outputs":[]},{"source":"# How many counties?\nThe shareholder is visiting the office today and heard you were done creating the view she requested. She stops by your desk to ask if you could tell her how many counties are present in the insights view.\n\nRemember:\n\n- The insights view is grouped by county.\n- You can use raw SQL to query the insights table.\n- You need to build something like SELECT ____ FROM insights and use it in the session.execute().all() method.\n- ____ should be replaced by the SQL function to count the number of rows…which means, the number of counties.\n- The session is already available and ready to use.","metadata":{},"cell_type":"markdown","id":"ce9f2b40-e915-4d73-9015-e515208ee44b"},{"source":"In [1]:\nsql_query = \"SELECT COUNT(DISTINCT county) FROM insights\"\nIn [2]:\nsession.execute(sql_query).all()\nOut[2]:\n[(26,)]","metadata":{},"cell_type":"code","id":"6177a898-95d7-4c9c-be6f-13bfb45afc3d","execution_count":null,"outputs":[]},{"source":"# Create a simple Excel file\n- Here is an Excel table. Let's create it using Python.\n\n- ![image](image.png)\n**Instructions**\n\n- Import the library you need to work with Excel files.\n- Create a new workbook.\n- Create a new worksheet from the current workbook.\n- Add Hello in cell (A1) and Datacamp in cell (B1).","metadata":{},"cell_type":"markdown","id":"fe4cab29-a369-410d-8d1b-3d0917d95d70"},{"source":"# Import the library needed\nimport xlsxwriter\n\n# Create a new Excel file\nworkbook = xlsxwriter.Workbook(\"insights.xlsx\")\n\n# Initialize a worksheet\nworksheet = workbook.add_worksheet()\n\n# Write the data in the current sheet\ndata = [\"Hello\", \"Datacamp\"]\nworksheet.write(0, 0, data[0])\nworksheet.write(1, 1, data[1])\n\n# Close the file\nworkbook.close()","metadata":{},"cell_type":"code","id":"8d781da7-04c7-4852-beb5-10ac198f0b6b","execution_count":null,"outputs":[]},{"source":"# Add a table into Excel file\n- You're now tasked with creating an Excel file containing an entire table. The end result should look like this:\n- ![image-2](image-2.png)\n","metadata":{},"cell_type":"markdown","id":"bd72e6dc-f08e-48de-8a92-2709be7780de"},{"source":"import xlsxwriter\n\nworkbook = xlsxwriter.Workbook(\"Products.xlsx\")\nworksheet = workbook.add_worksheet()\n    \n# Create a table with the available data in the current sheet\nworksheet.add_table(\n    \"B3:E8\",\n    {\n        \"data\": data,\n        \"columns\": [\n          \t# Use the appropriate names for the columns\n            {\"header\": \"id\"},\n             {\"header\": \"category\"},\n             {\"header\": \"name\"},\n             {\"header\": \"price\"},\n        ],\n    },\n)\n\n# Close the current file\nworkbook.close()","metadata":{},"cell_type":"code","id":"720f53fb-75a9-46b0-b151-9b20c60ed29a","execution_count":null,"outputs":[]},{"source":"# Export monthly insights\nYou're now ready to create a monthly report to be sent out to the shareholders. As a Business Analyst, you need to build the right query to retrieve the data and export it to an Excel file. Let's put everything together in a single script called insights_export.py, and generate monthly insights! The exported Excel file will be saved in the insights_export folder.\n\nRemember that for the previous month, and for each country, the shareholders want to know:\n\n- the number of sales\n- the total sales\n- the highest price a property sold for\n- the lowest price a property sold for\n- the average sales prices\n- The results are saved in the file called InsightsExport_YYYYMM.xlsx","metadata":{},"cell_type":"markdown","id":"acef9201-ba38-4171-bcca-22e7571a22ba"},{"source":"from datetime import datetime\nimport os\n\nfrom common.base import session\nfrom common.tables import PprCleanAll\nimport xlsxwriter\n\n\n# Settings\nbase_path = os.path.abspath(__file__ + \"/../../\")\nref_month = datetime.today().strftime(\"%Y-%m\")\n\nif __name__ == \"__main__\":\n    data = session.execute(\"SELECT * FROM insights\").all()\n    ref_month = datetime.today().strftime(\"%Y-%m\")\n    \n    # Create the workbook\n    workbook = xlsxwriter.Workbook(\n        f\"{base_path}/insights_export/InsightsExport_202102.xlsx\"\n    )\n    \n    # Add a new worksheet\n    worksheet = workbook.add_worksheet()\n    worksheet.set_column(\"B:G\", 12)\n    \n    # Add the table with all results in the newly created worksheet\n    worksheet.add_table(\n        \"B3:E20\",\n        {\n            \"data\": data,\n            \"columns\": [\n                {\"header\": \"County\"},\n                {\"header\": \"Number of Sales 3 month\"},\n                {\"header\": \"Tot sales 3 months\"},\n                {\"header\": \"Max sales 3 months\"},\n                {\"header\": \"Min sales 3 months\"},\n                {\"header\": \"Avg sales 3 months\"},\n            ],\n        },\n    )\n    workbook.close()\n    \n    print(\"Data exported:\", f\"{base_path}/insights_export/InsightsExport_202102.xlsx\")","metadata":{},"cell_type":"code","id":"d9877326-51af-49a2-bb02-2d68b33081dc","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}